\documentclass[a4paper]{article}
\usepackage{fullpage}
\usepackage{listings} %--- For including MATLAB code ---%
    \usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
    \definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
    \definecolor{mylilas}{RGB}{170,55,241}
    \lstset{language=Matlab,%
    basicstyle=\footnotesize\ttfamily,
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    numbers=left,%
    numberstyle={\tiny \color{black}},% size of the numbers
    numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},    
   }
%---End MATLAB code inclusion package ---%

\author{Ryan Day}
\title{Optimization Homework \#4}
\begin{document}
    \maketitle
    \section{Truss Optimization}

    \subsection{Scaling}
    Scaling the constraints is useful. I scaled it by dividing the constraints by $10^{2}$. This reduced the number of function calls by more than half when I implemented it. 
    I tried to make the constraints the same order of magnitude as the design functions.
    The design variables are all around the same order of magnitude, so I don't see the use of scaling there.
    
    \subsection{Matlab Code implementation}
    The matlab code is included in appendix \ref{appendix.trussCode}.
    I used a function that took in x, then perturbed the function with a step depending on a type input (forward, central, or complex), and then calculated the gradient and constraints gradient.
    It was not too hard to implement this function. 
    I simply added it to the end of obj and con in order to get the derivatvies.
    However, I had to make one change in order to get the complex function to work.
    This change was changing the inequality constraint from using the abs function to taking the square root of the value squared.
    This did the same thing as abs but could be used with complex variables.
    
    \subsection{Expected Errors of the derivatives}
    I expected the errors of the derivative to be greatest for the forward, then central, then complex. 
    The merits of the forward method is it is only takes one function call per derivative. 
    Central method is slightly more accurate, but takes twice as many function calls.
    Both central and forward methods have subtractive error which makes it so you can't have the step size be too small.
    The complex step method does not have this subtractive error which makes it so you can have an extremely small step size, but then you have to make sure your function can handle complex numbers.
    In addition to this, the computations of the complex step can take longer than the forward step because of the included complex numbers.
    
    I figured out the optimal perturbation of forward and central methods by comparing it to the complex step derivative.
    I checked compared the estimated derivatives from the first iteration between the complex step with a step size of $10^{-30}$. 
    I knew the complex derivative would be pretty accurate because it has no subtractive error, so you can make the step size extremely small. 
    $10^{-8}$ turned out to be the ideal step size for forward and central. It had an error for forward on the order of $10^{-6}$ which was fine.
    When I tried at 1e-9 there was an error in the gradient of forward of $10^{-4}$ which was too much. 
    For some reason the central wasn't converging when I scaled it, so that is why it was so much slower than the other functions.
    
    \subsection{Table and stopping criteria}
    \begin{center}
    \begin{tabular}[h]{c c c c c}
        & \# Function calls & \# Iterations & Avg Time execution& Final Objective value \\
        No Derivatives supplied&287&12&0.420 &1.5932e+03\\
        Forward method&309&12&0.415 & 1.5932e+03\\
        Central method&4621&5&1.114 & 1.5932e+03\\ 
        Complex method&287&12&0.510  &   1.5932e+03 
    \end{tabular}
\end{center}

The execution time was fastest by just a little bit with the forward method. It barely beat out the fmincon with no supplied derivatives.
The forward method and complex method had the same number of function calls as expected since they both call the objective function just once to calculate derivatives. 
The complex method took longer than the forward method because it had to deal with complex numbers. 
The central method took about twice as many function calls and so took a lot longer than any other method. 
However, it had about half the iterations, because it goes in a more accurate direction with .

\textbf{Stopping Criterion for no derivatives supplied: }

Optimization completed: The relative first-order optimality measure, 5.312214e-07,
is less than options.OptimalityTolerance = 1.000000e-06, and the relative maximum constraint
violation, 0.000000e+00, is less than options.ConstraintTolerance = 1.000000e-06.

\textbf{Stopping Criterion for other methods:}

\textbf{Forward:}

Optimization stopped because the relative changes in all elements of x are
less than options.StepTolerance = 1.000000e-10, and the relative maximum constraint
violation, 0.000000e+00, is less than options.ConstraintTolerance = 1.000000e-06.

\textbf{Central:}

Optimization completed: The relative first-order optimality measure, 7.486241e-07,
is less than options.OptimalityTolerance = 1.000000e-06, and the relative maximum constraint
violation, 0.000000e+00, is less than options.ConstraintTolerance = 1.000000e-06.

\textbf{Complex:}

Optimization completed: The relative first-order optimality measure, 3.928371e-08,
is less than options.OptimalityTolerance = 1.000000e-06, and the relative maximum constraint
violation, 0.000000e+00, is less than options.ConstraintTolerance = 1.000000e-06.

The stopping criterion for forward and without supplying derivatives was that the x inputs had very small changes, signalling that there was an optimum. 
For the complex and central step, the function output met the optimality measure. 
Because I only looked at 5 significant digits for the optimum, these measures didn't affect the optimum at all.

\section{Automatic Differentiation}
\subsection{How does this method work?}
I define the values to be of the valder class. 
Any other function that has a valder variable as part of its calculation will then take the valder variable  into account and then operator overloading is used to compute derivatives for the original valder functions.
In this way you can get derivatives without having to change your code much.
\subsection{How is AD different from other numerical methods?}
AD is different because it solves the derivatives analytically using lookup functions and operator overloading instead of numerically approximating the value.
\subsection{Matlab Code}
AD code is found in appendix \ref{appendix.adcode}
\appendix
\section{Truss Optimization}
\label{appendix.trussCode}
\lstinputlisting[language=matlab]{../OptimizeTruss/OptimizeTruss.m} 
\section{Automatic differentiation}
\lstinputlisting[language=matlab]{../AutomaticDifferentiation/optimize_spring.m} 
\label{appendix.adcode}
\end{document}
xopt =

  Columns 1 through 5

    6.3535    0.1016    6.4508    4.0730    0.1028

  Columns 6 through 10

    0.1016    5.4464    5.2882    6.0309    0.1039


fopt =

   1.4775e+03